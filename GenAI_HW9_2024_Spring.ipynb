{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yubinvip/Hung-yi-Lee-AI2024/blob/main/GenAI_HW9_2024_Spring.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MntmiANZCk2L"
      },
      "source": [
        "# GenAI HW9: Quick Summary of Lecture Video (演講影片快速摘要)\n",
        "## Objectives\n",
        "- ### Learn to quickly build applications related to speech recognition using existing APIs. (學習以現成的API快速搭建語音辨識相關的應用。)\n",
        "\n",
        "\n",
        "#### If you have any questions, please contact the TAs via TA hours, NTU COOL, or email to ntu-gen-ai-2024-spring-ta@googlegroups.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voEioD2DCoeq"
      },
      "source": [
        "# Part1 - Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSccLtt234Pm"
      },
      "source": [
        "## The lecture video provided for this assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgHVz9WF4Vfp"
      },
      "source": [
        "(1) For ease of processing, it has already been converted to a MP3 file.\n",
        "\n",
        "(2) If you would like to view the original video, the link is here:\n",
        "\n",
        "- 李琳山教授 信號與人生 (2023)\n",
        "\n",
        "  - https://www.youtube.com/watch?v=MxoQV4M0jY8\n",
        "\n",
        "\n",
        "(3) Since the original lecture video is quite long, we have edited the segment from 1:43:24 to 2:00:49 to use for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdoLJZE33oCD"
      },
      "source": [
        "## Install all necessary packages and import them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HREsIZV33yDy"
      },
      "source": [
        "The following code block takes about **150** seconds to run, but it may vary slightly depending on the condition of Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgL2wxdxCvA5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6726d94f-598f-4179-a79b-f5e20298236f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-z5ewkyne\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-z5ewkyne\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.4)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802826 sha256=f0ccbccbe03f68e4cbf44b054a904f9456ca6ddf6ce9d6d533eac8926b2c8a4e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kn5f1atq/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.6.0\n",
            "Collecting srt\n",
            "  Downloading srt-3.5.3.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: srt\n",
            "  Building wheel for srt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for srt: filename=srt-3.5.3-py3-none-any.whl size=22428 sha256=e3f2f947bd5b467905da55ff761bc46cdc70ecc126f43b7117772df4cc808b1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/31/a1/18e1e7e8bfdafd19e6803d7eb919b563dd11de380e4304e332\n",
            "Successfully built srt\n",
            "Installing collected packages: srt\n",
            "Successfully installed srt-3.5.3\n",
            "Collecting datetime\n",
            "  Downloading DateTime-5.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zope.interface (from datetime)\n",
            "  Downloading zope.interface-6.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.3/247.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from datetime) (2023.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface->datetime) (67.7.2)\n",
            "Installing collected packages: zope.interface, datetime\n",
            "Successfully installed datetime-5.5 zope.interface-6.3\n",
            "Collecting opencc\n",
            "  Downloading OpenCC-1.1.7-cp310-cp310-manylinux1_x86_64.whl (779 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.8/779.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencc\n",
            "Successfully installed opencc-1.1.7\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.0 dill-0.3.8 huggingface-hub-0.22.2 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython) (67.7.2)\n",
            "Collecting jedi>=0.16 (from IPython)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython) (0.2.13)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.19.1\n"
          ]
        }
      ],
      "source": [
        "# Install packages.\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install srt\n",
        "!pip install datetime\n",
        "!pip install opencc\n",
        "!pip install datasets\n",
        "!pip install numpy\n",
        "!pip install soundfile\n",
        "!pip install IPython\n",
        "!pip install openai\n",
        "!pip install -q -U google-generativeai\n",
        "!pip install anthropic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWqXz6C6omR9"
      },
      "source": [
        "The following code block takes about **5** seconds to run, but it may vary slightly depending on the condition of Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFwSa_x6C53S"
      },
      "outputs": [],
      "source": [
        "# Import packages.\n",
        "\n",
        "import whisper\n",
        "import srt\n",
        "import datetime\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "import pathlib\n",
        "import textwrap\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from opencc import OpenCC\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from openai import OpenAI\n",
        "import google.generativeai as genai\n",
        "import anthropic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFY6VDAyeooa"
      },
      "source": [
        "## Download data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBROu_HfgF1J"
      },
      "source": [
        "The code block below takes about **10** seconds to run, although there might be some slight variation depending on the state of Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAieqtY8evUJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217,
          "referenced_widgets": [
            "07fb96681a794a2a9b22fbf7c0a4f6db",
            "f34be7b48f8d4f1d8b2facdcd92760c6",
            "351df89abd2b4da9b608cbfc83dfaf55",
            "3bd2f0a1ca004d64b64b8af29a97d785",
            "a8e9182146344d099749e6309d567ba0",
            "c05d52994f9a44419971a981ac78ac86",
            "6b746f9ca5984260bf9af6c0aa9e24a5",
            "ae81ac0896e4450c8f81a700f1027bdf",
            "af14c439d0d648729e93522034cb3b25",
            "cb8f40f8a208435cb74102065b00a566",
            "423906d4a7d74189a5ac9b60bcdded2a",
            "54c07b1ebdeb4f6dbf8bda8b387b91e1",
            "33a929e2ebfc44428bce63a8dc91af9b",
            "46ca7fd4e5ca418e926b34650a99f695",
            "d0dfc1ea848a46e4b62ed9cf1baca771",
            "2e75157afa984d5e99272b7f527b22cb",
            "986955dac4644a7b9e2a7a58e09ed2fa",
            "56e89d31ac4e46b288157f37fd90b140",
            "eb27db333e2e407bb6afe143633e352f",
            "262761b6efe3441c9f89a2924764c4a1",
            "cf0b23e855fb45b580109a1a367913c7",
            "20414fae89ba4d1095c3680c84c5305f",
            "05fb2ea3c7724f0ebfcb5695557fff26",
            "d92966e4fcec4596bc24bd684fa63151",
            "8e3043bdcab6475c852d28607cdf79da",
            "ad3d4f71a3314d8c93be997b094d0bdf",
            "a77cf5367a2041218aba15342f528631",
            "3930aa2682464bc3b2f6cd77e0801487",
            "b1a1e562fb0044b6ae4bfb7d44cd1a2a",
            "f01756ebda6b433fa3fc34c87f3883d7",
            "ed88eba5dada4b5b8d8577d30a5f1354",
            "5975f20095a840dea0c2704a8ff6d1a1",
            "b7f80821c54e4e4bb396da127f29c71f"
          ]
        },
        "outputId": "4d70070d-77c3-43e9-f40a-03db1e34d724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/305 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07fb96681a794a2a9b22fbf7c0a4f6db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/3.14M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54c07b1ebdeb4f6dbf8bda8b387b91e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05fb2ea3c7724f0ebfcb5695557fff26"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load dataset.\n",
        "dataset_name = \"kuanhuggingface/NTU-GenAI-2024-HW9\"\n",
        "dataset = load_dataset(dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pN3dOGyrI-"
      },
      "source": [
        "The code block below takes about **15** seconds to run, although there might be some slight variation depending on the state of Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E68E8Ej2isAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79416dbd-fea4-48c7-f908-046f173e96fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now, we are going to transcribe the audio: 李琳山教授 信號與人生 (2023) (ntu-gen-ai-2024-hw9-16k.mp3).\n"
          ]
        }
      ],
      "source": [
        "# Prepare audio.\n",
        "input_audio = dataset[\"test\"][\"audio\"][0]\n",
        "input_audio_name = input_audio[\"path\"]\n",
        "input_audio_array = input_audio[\"array\"].astype(np.float32)\n",
        "sampling_rate = input_audio[\"sampling_rate\"]\n",
        "\n",
        "print(f\"Now, we are going to transcribe the audio: 李琳山教授 信號與人生 (2023) ({input_audio_name}).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxTn1CfzDCXy"
      },
      "source": [
        "# Part2 - Automatic Speech Recognition (ASR)\n",
        "The function \"speech_recognition\" aims to convert audio to subtitle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmWjjLUGC9z3"
      },
      "outputs": [],
      "source": [
        "def speech_recognition(model_name, input_audio, output_subtitle_path, decode_options, cache_dir=\"./\"):\n",
        "    '''\n",
        "        (1) Objective:\n",
        "            - This function aims to convert audio to subtitle.\n",
        "\n",
        "        (2) Arguments:\n",
        "\n",
        "            - model_name (str):\n",
        "                The name of the model. There are five model sizes, including tiny, base, small, medium, large-v3.\n",
        "                For example, you can use 'tiny', 'base', 'small', 'medium', 'large-v3' to specify the model name.\n",
        "                You can see 'https://github.com/openai/whisper' for more details.\n",
        "\n",
        "            - input_audio (Union[str, np.ndarray, torch.Tensor]):\n",
        "                The path to the audio file to open, or the audio waveform\n",
        "                - For example, if your input audio path is 'input.wav', you can use 'input.wav' to specify the input audio path.\n",
        "                - For example, if your input audio array is 'audio_array', you can use 'audio_array' to specify the input audio array.\n",
        "\n",
        "            - output_subtitle_path (str):\n",
        "                The path of the output subtitle file.\n",
        "                For example, if you want to save the subtitle file as 'output.srt', you can use 'output.srt' to specify the output subtitle path.\n",
        "\n",
        "            - decode_options (dict):\n",
        "                The options for decoding the audio file, including 'initial_prompt', 'prompt', 'prefix', 'temperature'.\n",
        "                - initial_prompt (str):\n",
        "                    Optional text to provide as a prompt for the first window. This can be used to provide, or\n",
        "                    \"prompt-engineer\" a context for transcription, e.g. custom vocabularies or proper nouns\n",
        "                    to make it more likely to predict those word correctly.\n",
        "                    Default: None.\n",
        "\n",
        "                You can see \"https://github.com/openai/whisper/blob/main/whisper/decoding.py\" and \"https://github.com/openai/whisper/blob/main/whisper/transcribe.py\"\n",
        "                for more details.\n",
        "\n",
        "                - temperature (float):\n",
        "                    The temperature for sampling from the model. Higher values mean more randomness.\n",
        "                    Default: 0.0\n",
        "\n",
        "            - cache_dir (str):\n",
        "                The path of the cache directory for saving the model.\n",
        "                For example, if you want to save the cache files in 'cache' directory, you can use 'cache' to specify the cache directory.\n",
        "                Default: './'\n",
        "\n",
        "        (3) Example:\n",
        "\n",
        "            - If you want to use the 'base' model to convert 'input.wav' to 'output.srt' and save the cache files in 'cache' directory,\n",
        "            you can call this function as follows:\n",
        "\n",
        "                speech_recognition(model_name='base', input_audio_path='input.wav', output_subtitle_path='output.srt', cache_dir='cache')\n",
        "    '''\n",
        "\n",
        "    # Record the start time.\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"=============== Loading Whisper-{model_name} ===============\")\n",
        "\n",
        "    # Load the model.\n",
        "    model = whisper.load_model(name=model_name, download_root=cache_dir)\n",
        "\n",
        "    print(f\"Begin to utilize Whisper-{model_name} to transcribe the audio.\")\n",
        "\n",
        "    # Transcribe the audio.\n",
        "    transcription = model.transcribe(audio=input_audio, language=decode_options[\"language\"], verbose=False,\n",
        "                                     initial_prompt=decode_options[\"initial_prompt\"], temperature=decode_options[\"temperature\"])\n",
        "\n",
        "    # Record the end time.\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(f\"The process of speech recognition costs {end_time - start_time} seconds.\")\n",
        "\n",
        "    subtitles = []\n",
        "    # Convert the transcription to subtitle and iterate over the segments.\n",
        "    for i, segment in tqdm(enumerate(transcription[\"segments\"])):\n",
        "\n",
        "        # Convert the start time to subtitle format.\n",
        "        start_time = datetime.timedelta(seconds=segment[\"start\"])\n",
        "\n",
        "        # Convert the end time to subtitle format.\n",
        "        end_time = datetime.timedelta(seconds=segment[\"end\"])\n",
        "\n",
        "        # Get the subtitle text.\n",
        "        text = segment[\"text\"]\n",
        "\n",
        "        # Append the subtitle to the subtitle list.\n",
        "        subtitles.append(srt.Subtitle(index=i, start=start_time, end=end_time, content=text))\n",
        "\n",
        "    # Convert the subtitle list to subtitle content.\n",
        "    srt_content = srt.compose(subtitles)\n",
        "\n",
        "    print(f\"\\n=============== Saving the subtitle to {output_subtitle_path} ===============\")\n",
        "\n",
        "    # Save the subtitle content to the subtitle file.\n",
        "    with open(output_subtitle_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(srt_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3ZkyefXpvmh"
      },
      "source": [
        "In the following block, you can modify your desired parameters and the path of input file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UULEr1GpDAl6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Parameter Setting of Whisper { run: \"auto\" }\n",
        "\n",
        "''' In this block, you can modify your desired parameters and the path of input file. '''\n",
        "\n",
        "# The name of the model you want to use.\n",
        "# For example, you can use 'tiny', 'base', 'small', 'medium', 'large-v3' to specify the model name.\n",
        "# @markdown **model_name**: The name of the model you want to use.\n",
        "model_name = \"medium\" # @param [\"tiny\", \"base\", \"small\", \"medium\", \"large-v3\"]\n",
        "\n",
        "# Define the suffix of the output file.\n",
        "# @markdown **suffix**: The output file name is \"output-{suffix}.* \", where .* is the file extention (.txt or .srt)\n",
        "suffix = \"信號與人生\" # @param {type: \"string\"}\n",
        "\n",
        "# Path to the output file.\n",
        "output_subtitle_path = f\"./output-{suffix}.srt\"\n",
        "\n",
        "# Path of the output raw text file from the SRT file.\n",
        "output_raw_text_path = f\"./output-{suffix}.txt\"\n",
        "\n",
        "# Path to the directory where the model and dataset will be cached.\n",
        "cache_dir = \"./\"\n",
        "\n",
        "# The language of the lecture video.\n",
        "# @markdown **language**: The language of the lecture video.\n",
        "language = \"zh\" # @param {type:\"string\"}\n",
        "\n",
        "# Optional text to provide as a prompt for the first window.\n",
        "# @markdown **initial_prompt**: Optional text to provide as a prompt for the first window.\n",
        "initial_prompt = \"請用繁體中文\" #@param {type:\"string\"}\n",
        "\n",
        "# The temperature for sampling from the model. Higher values mean more randomness.\n",
        "# @markdown  **temperature**: The temperature for sampling from the model. Higher values mean more randomness.\n",
        "temperature = 0 # @param {type:\"slider\", min:0, max:1, step:0.1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBhoPRKR9S4w"
      },
      "outputs": [],
      "source": [
        "# Construct DecodingOptions\n",
        "decode_options = {\n",
        "    \"language\": language,\n",
        "    \"initial_prompt\": initial_prompt,\n",
        "    \"temperature\": temperature\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SfQ5Xn-fjya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab1892a-18e1-40e6-b5fc-ee5e82c769af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting: (1) Model: whisper-medium (2) Language: zh (2) Initial Prompt: 請用繁體中文 (3) Temperature: 0\n",
            "Transcribe 李琳山教授 信號與人生 (2023)\n"
          ]
        }
      ],
      "source": [
        "# print message.\n",
        "message = \"Transcribe 李琳山教授 信號與人生 (2023)\"\n",
        "print(f\"Setting: (1) Model: whisper-{model_name} (2) Language: {language} (2) Initial Prompt: {initial_prompt} (3) Temperature: {temperature}\")\n",
        "print(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxgZ2DNgpGlO"
      },
      "source": [
        "The code block below takes about **90 (240)** seconds to run when using the **base (medium)** model and **a T4 GPU**, although there might be some slight variation depending on the state of Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOULGnw5RF6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb4f6db3-4d2c-4e73-b77a-d46ee8ce695d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============== Loading Whisper-medium ===============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████| 1.42G/1.42G [00:12<00:00, 122MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin to utilize Whisper-medium to transcribe the audio.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 104500/104500 [02:45<00:00, 631.16frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The process of speech recognition costs 196.35759162902832 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "370it [00:00, 169069.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=============== Saving the subtitle to ./output-信號與人生.srt ===============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Running ASR.\n",
        "speech_recognition(model_name=model_name, input_audio=input_audio_array, output_subtitle_path=output_subtitle_path, decode_options=decode_options, cache_dir=cache_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgmFtnti1qhU"
      },
      "source": [
        "You can check the result of automatic speech recognition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeU54f5X1erZ"
      },
      "outputs": [],
      "source": [
        "''' Open the SRT file and read its content.\n",
        "The format of SRT is:\n",
        "\n",
        "[Index]\n",
        "[Begin time] (hour:minute:second) --> [End time] (hour:minute:second)\n",
        "[Transcription]\n",
        "\n",
        "'''\n",
        "\n",
        "with open(output_subtitle_path, 'r', encoding='utf-8') as file:\n",
        "    content = file.read()\n",
        "\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7JcN-kUDE_g"
      },
      "source": [
        "# Part3 - Preprocess the results of automatic speech recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2R40faVDShf"
      },
      "outputs": [],
      "source": [
        "def extract_and_save_text(srt_filename, output_filename):\n",
        "\n",
        "    '''\n",
        "    (1) Objective:\n",
        "        - This function extracts the text from an SRT file and saves it to a new text file.\n",
        "        - It also converts the Simplified Chinese to Traditional Chinese.\n",
        "\n",
        "    (2) Arguments:\n",
        "\n",
        "        - srt_filename: The path to the SRT file.\n",
        "\n",
        "        - output_filename: The name of the output text file.\n",
        "\n",
        "    (3) Example:\n",
        "        - If your SRT file is named 'subtitle.srt' and you want to save the extracted text to a file named 'output.txt', you can use the function like this:\n",
        "            extract_and_save_text('subtitle.srt', 'output.txt')\n",
        "\n",
        "    '''\n",
        "\n",
        "    # Open the SRT file and read its content.\n",
        "    with open(srt_filename, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Use regular expression to remove the timecode.\n",
        "    pure_text = re.sub(r'\\d+\\n\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}\\n', '', content)\n",
        "\n",
        "    # Remove the empty lines.\n",
        "    pure_text = re.sub(r'\\n\\n+', '\\n', pure_text)\n",
        "\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    pure_text_conversion = cc.convert(pure_text)\n",
        "\n",
        "    # Write the extracted text to a new file.\n",
        "    with open(output_filename, 'w', encoding='utf-8') as output_file:\n",
        "        output_file.write(pure_text_conversion)\n",
        "\n",
        "    print(f'Extracted text has been saved to {output_filename}.\\n\\n')\n",
        "\n",
        "    return pure_text_conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWDl1vuADd0e"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, max_length):\n",
        "    \"\"\"\n",
        "    (1) Objective:\n",
        "        - This function is used to split a long string into smaller strings of a specified length.\n",
        "\n",
        "    (2) Arguments:\n",
        "        - text: str, the long string to be split.\n",
        "        - max_length: int, the maximum length of each smaller string.\n",
        "\n",
        "    (3) Returns:\n",
        "        - split_text: list, a list of smaller strings.\n",
        "\n",
        "    (3) Example:\n",
        "        - If you want to split a string named \"long_string\" into smaller strings of length 100, you can use the function like this:\n",
        "            chunk_text(long_string, 100)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    return textwrap.wrap(text, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO01S41pOsSP"
      },
      "outputs": [],
      "source": [
        "''' In this block, you can modify your desired parameters and the path of input file. '''\n",
        "\n",
        "# # The length of the text chunks.\n",
        "chunk_length = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-PpbkoS-5bI"
      },
      "outputs": [],
      "source": [
        "# Extracts the text from an SRT file and saves it to a new text file\n",
        "pure_text = extract_and_save_text(srt_filename=output_subtitle_path, output_filename=output_raw_text_path)\n",
        "\n",
        "# Split a long document into smaller chunks of a specified length\n",
        "chunks = chunk_text(text=pure_text, max_length=512)\n",
        "\n",
        "# You can see the number of words and contents in each paragraph.\n",
        "print(\"Review the results of splitting the long text into several short texts.\\n\")\n",
        "for index, chunk in enumerate(chunks):\n",
        "    if index == 0:\n",
        "        print(f\"\\n========== The {index + 1}-st segment of the split ({len(chunk)} words) ==========\\n\\n\")\n",
        "        for text in textwrap.wrap(chunk, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "    elif index == 1:\n",
        "        print(f\"\\n========== The {index + 1}-nd segment of the split ({len(chunk)} words) ==========\\n\\n\")\n",
        "        for text in textwrap.wrap(chunk, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "    elif index == 2:\n",
        "        print(f\"\\n========== The {index + 1}-rd segment of the split ({len(chunk)} words) ==========\\n\\n\")\n",
        "        for text in textwrap.wrap(chunk, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "    else:\n",
        "        print(f\"\\n========== The {index + 1}-th segment of the split ({len(chunk)} words) ==========\\n\\n\")\n",
        "        for text in textwrap.wrap(chunk, 80):\n",
        "            print(f\"{text}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wuvx30fkW4kU"
      },
      "source": [
        "# Part4 - Summarization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mr9Kz634zT2"
      },
      "source": [
        "## **You only need to choose one of the following parts.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCPYjOAyXWaE"
      },
      "source": [
        "## **If you want to use ChatGPT, begin with this part.**\n",
        "##### (1) You can refer to https://shorturl.at/X0NDY (Page 44) for obtaining ChatGPT API key.\n",
        "##### (2) You can refer to https://platform.openai.com/docs/models/overview for more details about models you can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCara20SW8AN"
      },
      "outputs": [],
      "source": [
        "def summarization(client, summarization_prompt, model_name=\"gpt-3.5-turbo\", temperature=0.0, top_p=1.0, max_tokens=512):\n",
        "    \"\"\"\n",
        "    (1) Objective:\n",
        "        - Use the OpenAI Chat API to summarize a given text.\n",
        "\n",
        "    (2) Arguments:\n",
        "        - client: OpenAI Chat API client.\n",
        "        - summarization_prompt: The summarization prompt including the text which need to be summarized.\n",
        "        - model_name: The model name, default is \"gpt-3.5-turbo\". You can refer to \"https://platform.openai.com/docs/models/overview\" for more details.\n",
        "        - temperature: Controls randomness in the response. Lower values make responses more deterministic, default is 0.0.\n",
        "        - top_p: Controls diversity via nucleus sampling. Higher values lead to more diverse responses, default is 1.0.\n",
        "        - max_tokens: The maximum number of tokens to generate in the completion, default is 512.\n",
        "\n",
        "    (3) Return:\n",
        "        - The summarized text.\n",
        "\n",
        "    (4) Example:\n",
        "        - If the text is \"ABC\" and the summarization prompt is \"DEF\", system_prompt is \"GHI\", model_name is \"gpt-3.5-turbo\",\n",
        "          temperature is 0.0, top_p is 1.0, and max_tokens is 512, then you can call the function like this:\n",
        "\n",
        "              summarization(client=client, text=\"ABC\", summarization_prompt=\"DEF\", system_prompt=\"GHI\", model_name=\"gpt-3.5-turbo\", temperature=0.0, top_p=1.0, max_tokens=512)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # The user prompt is a concatenation of the summarization_prompt and text.\n",
        "    user_prompt = summarization_prompt\n",
        "\n",
        "    while True:\n",
        "\n",
        "        try:\n",
        "            # Use the OpenAI Chat API to summarize the text.\n",
        "            chat_completion = client.chat.completions.create(\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": user_prompt,\n",
        "                    }\n",
        "                ],\n",
        "                    model=model_name,\n",
        "                    temperature=temperature,\n",
        "                    top_p=top_p,\n",
        "                    max_tokens=max_tokens\n",
        "            )\n",
        "\n",
        "            break\n",
        "\n",
        "        except:\n",
        "            # If the API call fails, wait for 1 second and try again.\n",
        "            print(\"The API call fails, wait for 1 second and try again.\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    return chat_completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3VZeUfBYcih"
      },
      "outputs": [],
      "source": [
        "# @title Parameter Setting of ChatGPT { run: \"auto\" }\n",
        "''' ===== In this block, you can modify your desired parameters and set your OpenAI API key ===== '''\n",
        "\n",
        "# Your OpenAI API key.\n",
        "# @markdown **openai_api_key**: Your OpenAI API key.\n",
        "openai_api_key = \"YOUR_OPENAI_API_KEY\" # @param {type:\"string\"}\n",
        "\n",
        "# The model name, default is \"gpt-3.5-turbo\". You can refer to \"https://platform.openai.com/docs/models/overview\" for more details.\n",
        "# @markdown **model_name**: The model name, default is \"gpt-3.5-turbo\". You can refer to \"https://platform.openai.com/docs/models/overview\" for more details.\n",
        "model_name = \"gpt-3.5-turbo\" # @param {type: \"string\"}\n",
        "\n",
        "# Controls randomness in the response. Lower values make responses more deterministic.\n",
        "# @markdown **temperature**: Controls randomness in the response. Lower values make responses more deterministic.\n",
        "temperature = 0 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "# Controls diversity via nucleus sampling. Higher values lead to more diverse responses.\n",
        "# @markdown **top_p**: Controls diversity via nucleus sampling. Higher values lead to more diverse responses.\n",
        "top_p = 0 # @param {type:\"slider\", min:0, max:1, step:0.1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysnuD_eIeWjY"
      },
      "outputs": [],
      "source": [
        "# Construct openai client.\n",
        "client = OpenAI(api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwDxai-YY4Hl"
      },
      "source": [
        "The code block below takes about **30** seconds to run when using the **gpt-3.5-turbo** model, but the actual time may vary depending on the condition of Colab and the status of the OpenAI API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvCBfcW7Qd-e"
      },
      "source": [
        "### We offer the following two methods for summarization.\n",
        "Reference: https://reurl.cc/VzagLA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9QC8lG_QRZL"
      },
      "source": [
        "#### **If you want to use the method of Multi-Stage Summarization, begin with this part.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUma_oXAQtmg"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of ChatGPT Multi-Stage Summarization: Paragraph { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# The maximum number of tokens to generate in the completion.\n",
        "# @markdown **max_tokens**: The maximum number of tokens to generate in the completion.\n",
        "max_tokens = 350 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown #### Changing **summarization_prompt_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "summarization_prompt_template = \"用 300 個字內寫出這段文字的摘要，其中包括要點和所有重要細節：<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQtk04XATy1j"
      },
      "source": [
        "##### Step1: Split the long text into multiple smaller pieces and obtain summaries for each smaller text piece separately"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUTFgIGl1IRp"
      },
      "source": [
        "The code block below takes about **80** seconds to run when using the (1) **gpt-3.5-turbo** model, (2) length of chunks is 512 and (3) maximum number of tokens is 250, but the actual time may vary depending on the condition of Colab and the status of the OpenAI API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nv-Ko3UZYjpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9513947-a9c4-4915-c6e8-8fe6f3fb221d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------Summary of Segment 1----------------------------\n",
            "\n",
            "學問是需要透過實際做來獲得的。單純地聽或閱讀可能無法真正吸收知識。透過完成作業或專案，將所學知識整合並應用，才能真正理解和掌握。舉例來說，完成一個專案或寫一個程\n",
            "\n",
            "式可以幫助我們將學到的知識融會貫通。因此，我們應該多動手多做，讓學問真正成為自己的。成績單上的分數並不一定能完全反映我們對知識的掌握程度。\n",
            "\n",
            "Length of summary for segment 1: 149\n",
            "Time taken to generate summary for segment 1: 4.82 sec.\n",
            "\n",
            "----------------------------Summary of Segment 2----------------------------\n",
            "\n",
            "許多人認為某門課不重要，甚至選擇不修或不做 final project，因為覺得太累或不必要。然而，這些看似無用的事實際上是學習的機會，能夠讓你全面成長。即使做\n",
            "\n",
            "了很多辛苦的事情可能沒有具體成果，但對於全面學習和思考能力的培養卻是很有幫助的。重要的是要不漏掉這些機會，並在讀書時思考數學式子和概念，培養自己的思考能力和習慣\n",
            "\n",
            "。這樣才能真正理解所學知識，並提升自己的思考能力。\n",
            "\n",
            "Length of summary for segment 2: 185\n",
            "Time taken to generate summary for segment 2: 5.35 sec.\n",
            "\n",
            "----------------------------Summary of Segment 3----------------------------\n",
            "\n",
            "學習不僅僅存在於課業之中，課業外也有很多可以學習的地方。學習是一種增長、進步和獲得快樂的過程。例如，打球可以提升健康、手腦協調、團隊精神和人際互動能力；爬山可以\n",
            "\n",
            "增加見識和學習到很多東西；旅行也可以擴展視野和增加經驗。無論是什麼活動，只要讓你感到快樂並有所增長和進步，都值得花時間和精力去學習。所以，把這些課外活動看成是學\n",
            "\n",
            "習的機會，都是值得努力去做的事情。\n",
            "\n",
            "Length of summary for segment 3: 177\n",
            "Time taken to generate summary for segment 3: 6.07 sec.\n",
            "\n",
            "----------------------------Summary of Segment 4----------------------------\n",
            "\n",
            "談戀愛和交朋友都是學習人際互動和溝通的好機會，即使沒有緣分也可以透過交朋友來學習。在大學裡，有很多機會參加各種活動，例如戲劇社或舞蹈社，這些活動可以幫助我們成長\n",
            "\n",
            "和進步。即使不是表演者，幕後工作也同樣重要，例如規劃或軟體組。在電機系裡，有很多優秀的同學可以成為好朋友，所以努力交朋友是有幫助的。透過參加各種活動，我們可以學\n",
            "\n",
            "到很多人與人之間的互動和溝通技巧，這對我們的成長和發展都是很重要的。\n",
            "\n",
            "Length of summary for segment 4: 194\n",
            "Time taken to generate summary for segment 4: 6.48 sec.\n",
            "\n",
            "----------------------------Summary of Segment 5----------------------------\n",
            "\n",
            "這段文字談論到不僅在電機工程領域，其他活動也能帶來成長和進步，包括戲劇學會和校內外的各種活動。儘管這些活動沒有考試或成績，但它們對個人發展非常重要。在今天的電機\n",
            "\n",
            "工程中，成功很少是一個人完成的，需要與他人合作。學習如何進入團隊、成為領導者並推動想做的事情是必要的。這些被稱為軟實力，是除了硬實力外的重要技能。\n",
            "\n",
            "Length of summary for segment 5: 153\n",
            "Time taken to generate summary for segment 5: 5.43 sec.\n",
            "\n",
            "----------------------------Summary of Segment 6----------------------------\n",
            "\n",
            "成功與否不僅取決於硬實力，更重要的是軟實力，包括人際交往能力、溝通能力、領導能力等。這些soft skills對於成功至關重要，並且可以透過課外活動和學習機會來\n",
            "\n",
            "培養。雖然有些人天生具備這些能力，但大多數人需要透過努力培養。作者分享自己在大學期間透過努力培養soft\n",
            "\n",
            "skills的經驗，強調這些能力對於電機工程師的成功至關重要。\n",
            "\n",
            "Length of summary for segment 6: 164\n",
            "Time taken to generate summary for segment 6: 4.39 sec.\n",
            "\n",
            "----------------------------Summary of Segment 7----------------------------\n",
            "\n",
            "在35歲到55歲這20年間被視為人生的黃金時期，這段時間是最好的發展時期。在這個時期之前，可能各方面尚未具備，還沒有完全訓練好；而之後，年紀大了可能會有一些限制\n",
            "\n",
            "。在電機系的畢業同學中，有些人的發展呈現斜率逐漸趨緩，有些人開始晚但斜率較高，也有些人起步快但後來趨於平緩。每個人的情況不同，但實力、努力、大智和self\n",
            "\n",
            "skill是影響發展的主要因素，而不僅僅是學業成績。最終，重要的是這四個因素如何影響個人的發展。\n",
            "\n",
            "Length of summary for segment 7: 206\n",
            "Time taken to generate summary for segment 7: 5.85 sec.\n",
            "\n",
            "----------------------------Summary of Segment 8----------------------------\n",
            "\n",
            "這段文字主要談到了實力、努力、大智和self skills 四個重要的要素。實力是指在電機工程領域中各種技能和知識的總和，要盡量做全面的學習以避免overfit\n",
            "\n",
            "ting。努力是每個人都應該具備的品質，對未來的發展有著重要影響。self skills\n",
            "\n",
            "指的是平常被忽視但卻很重要的技能，通過在課業外的事情上增長進步可以讓自己變得更強大。此外，每個人都應該有自己的長程目標，這將有助於指導未來的努力和發展方向。\n",
            "\n",
            "Length of summary for segment 8: 203\n",
            "Time taken to generate summary for segment 8: 5.43 sec.\n",
            "\n",
            "----------------------------Summary of Segment 9----------------------------\n",
            "\n",
            "我希望在未來的5年、10年、15年甚至更長的時間裡，專注於努力實現我想要做的事情，這是我的長程目標。如果我認為這些事情對我來說非常有意義，我願意花時間和精力去實\n",
            "\n",
            "現。擁有這種長程目標的人更容易朝著成功的方向前進。對我來說，這四件事情是真正影響我未來的關鍵。\n",
            "\n",
            "Length of summary for segment 9: 127\n",
            "Time taken to generate summary for segment 9: 4.49 sec.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "paragraph_summarizations = []\n",
        "\n",
        "# First, we summarize each section that has been split up separately.\n",
        "for index, chunk in enumerate(chunks):\n",
        "\n",
        "    # Record the start time.\n",
        "    start = time.time()\n",
        "\n",
        "    # Construct summarization prompt.\n",
        "    summarization_prompt = summarization_prompt_template.replace(\"<text>\", chunk)\n",
        "\n",
        "    # We summarize each section that has been split up separately.\n",
        "    response = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "    # Calculate the execution time and round it to 2 decimal places.\n",
        "    cost_time = round(time.time() - start, 2)\n",
        "\n",
        "    # Print the summary and its length.\n",
        "    print(f\"----------------------------Summary of Segment {index + 1}----------------------------\\n\")\n",
        "    for text in textwrap.wrap(response, 80):\n",
        "        print(f\"{text}\\n\")\n",
        "    print(f\"Length of summary for segment {index + 1}: {len(response)}\")\n",
        "    print(f\"Time taken to generate summary for segment {index + 1}: {cost_time} sec.\\n\")\n",
        "\n",
        "    # Record the result.\n",
        "    paragraph_summarizations.append(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6QXhSK_eACs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ff2bf63-36c3-47f9-d301-9eef0d4da28d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of segment 1: 學問是需要透過實際做來獲得的。單純地聽或閱讀可能無法真正吸收知識。透過完成作業或專案，將所學知識整合並應用，才能真正理解和掌握。舉例來說，完成一個專案或寫一個程式可以幫助我們將學到的知識融會貫通。因此，我們應該多動手多做，讓學問真正成為自己的。成績單上的分數並不一定能完全反映我們對知識的掌握程度。\n",
            "Summary of segment 2: 許多人認為某門課不重要，甚至選擇不修或不做 final project，因為覺得太累或不必要。然而，這些看似無用的事實際上是學習的機會，能夠讓你全面成長。即使做了很多辛苦的事情可能沒有具體成果，但對於全面學習和思考能力的培養卻是很有幫助的。重要的是要不漏掉這些機會，並在讀書時思考數學式子和概念，培養自己的思考能力和習慣。這樣才能真正理解所學知識，並提升自己的思考能力。\n",
            "Summary of segment 3: 學習不僅僅存在於課業之中，課業外也有很多可以學習的地方。學習是一種增長、進步和獲得快樂的過程。例如，打球可以提升健康、手腦協調、團隊精神和人際互動能力；爬山可以增加見識和學習到很多東西；旅行也可以擴展視野和增加經驗。無論是什麼活動，只要讓你感到快樂並有所增長和進步，都值得花時間和精力去學習。所以，把這些課外活動看成是學習的機會，都是值得努力去做的事情。\n",
            "Summary of segment 4: 談戀愛和交朋友都是學習人際互動和溝通的好機會，即使沒有緣分也可以透過交朋友來學習。在大學裡，有很多機會參加各種活動，例如戲劇社或舞蹈社，這些活動可以幫助我們成長和進步。即使不是表演者，幕後工作也同樣重要，例如規劃或軟體組。在電機系裡，有很多優秀的同學可以成為好朋友，所以努力交朋友是有幫助的。透過參加各種活動，我們可以學到很多人與人之間的互動和溝通技巧，這對我們的成長和發展都是很重要的。\n",
            "Summary of segment 5: 這段文字談論到不僅在電機工程領域，其他活動也能帶來成長和進步，包括戲劇學會和校內外的各種活動。儘管這些活動沒有考試或成績，但它們對個人發展非常重要。在今天的電機工程中，成功很少是一個人完成的，需要與他人合作。學習如何進入團隊、成為領導者並推動想做的事情是必要的。這些被稱為軟實力，是除了硬實力外的重要技能。\n",
            "Summary of segment 6: 成功與否不僅取決於硬實力，更重要的是軟實力，包括人際交往能力、溝通能力、領導能力等。這些soft skills對於成功至關重要，並且可以透過課外活動和學習機會來培養。雖然有些人天生具備這些能力，但大多數人需要透過努力培養。作者分享自己在大學期間透過努力培養soft skills的經驗，強調這些能力對於電機工程師的成功至關重要。\n",
            "Summary of segment 7: 在35歲到55歲這20年間被視為人生的黃金時期，這段時間是最好的發展時期。在這個時期之前，可能各方面尚未具備，還沒有完全訓練好；而之後，年紀大了可能會有一些限制。在電機系的畢業同學中，有些人的發展呈現斜率逐漸趨緩，有些人開始晚但斜率較高，也有些人起步快但後來趨於平緩。每個人的情況不同，但實力、努力、大智和self skill是影響發展的主要因素，而不僅僅是學業成績。最終，重要的是這四個因素如何影響個人的發展。\n",
            "Summary of segment 8: 這段文字主要談到了實力、努力、大智和self skills 四個重要的要素。實力是指在電機工程領域中各種技能和知識的總和，要盡量做全面的學習以避免overfitting。努力是每個人都應該具備的品質，對未來的發展有著重要影響。self skills 指的是平常被忽視但卻很重要的技能，通過在課業外的事情上增長進步可以讓自己變得更強大。此外，每個人都應該有自己的長程目標，這將有助於指導未來的努力和發展方向。\n",
            "Summary of segment 9: 我希望在未來的5年、10年、15年甚至更長的時間裡，專注於努力實現我想要做的事情，這是我的長程目標。如果我認為這些事情對我來說非常有意義，我願意花時間和精力去實現。擁有這種長程目標的人更容易朝著成功的方向前進。對我來說，這四件事情是真正影響我未來的關鍵。\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# First, we collect all the summarizations obtained before and print them.\n",
        "\n",
        "collected_summarization = \"\"\n",
        "for index, paragraph_summarization in enumerate(paragraph_summarizations):\n",
        "    collected_summarization += f\"Summary of segment {index + 1}: {paragraph_summarization}\\n\"\n",
        "\n",
        "print(collected_summarization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itdqp3H5T2T7"
      },
      "source": [
        "##### Step2: After obtaining summaries for each smaller text piece separately, process these summaries to generate the final summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0ghZaKfVEyN"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of ChatGPT Multi-Stage Summarization: Total { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# We set the maximum number of tokens to ensure that the final summary does not exceed 550 tokens.\n",
        "# @markdown **max_tokens**: We set the maximum number of tokens to ensure that the final summary does not exceed 550 tokens.\n",
        "max_tokens = 550 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Changing **summarization_prompt_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "summarization_prompt_template = \"在 500 字以內寫出以下文字的簡潔摘要：<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka0JcOvYVIWu"
      },
      "source": [
        "The code block below takes about **10** seconds to run when using the (1) **gpt-3.5-turbo** model and (2) maximum number of tokens is 500, but the actual time may vary depending on the condition of Colab and the status of the OpenAI API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zHPMRDWeCuq"
      },
      "outputs": [],
      "source": [
        "# Finally, we compile a final summary from the summaries of each section.\n",
        "\n",
        "# Record the start time.\n",
        "start = time.time()\n",
        "\n",
        "# Run final summarization.\n",
        "summarization_prompt = summarization_prompt_template.replace(\"<text>\", collected_summarization)\n",
        "final_summarization = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "# Calculate the execution time and round it to 2 decimal places.\n",
        "cost_time = round(time.time() - start, 2)\n",
        "\n",
        "# Print the summary and its length.\n",
        "print(f\"----------------------------Final Summary----------------------------\\n\")\n",
        "for text in textwrap.wrap(final_summarization, 80):\n",
        "        print(f\"{text}\")\n",
        "print(f\"\\nLength of final summary: {len(final_summarization)}\")\n",
        "print(f\"Time taken to generate the final summary: {cost_time} sec.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi7eERB8eGdK"
      },
      "outputs": [],
      "source": [
        "''' In this block, you can modify your desired output path of final summary. '''\n",
        "\n",
        "output_path = f\"./final-summary-{suffix}-chatgpt-multi-stage.txt\"\n",
        "\n",
        "# If you need to convert Simplified Chinese to Traditional Chinese, please set this option to True; otherwise, set it to False.\n",
        "convert_to_tradition_chinese = False\n",
        "\n",
        "if convert_to_tradition_chinese == True:\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    final_summarization = cc.convert(final_summarization)\n",
        "\n",
        "# Output your final summary\n",
        "with open(output_path, \"w\") as fp:\n",
        "    fp.write(final_summarization)\n",
        "\n",
        "# Show the result.\n",
        "print(f\"Final summary has been saved to {output_path}\")\n",
        "print(f\"\\n===== Below is the final summary ({len(final_summarization)} words) =====\\n\")\n",
        "for text in textwrap.wrap(final_summarization, 64):\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRzf_0cTV6TS"
      },
      "source": [
        "#### **If you want to use the method of Refinement, begin with this part.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k79C13kWW_Ye"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of ChatGPT Refinement { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# We set the maximum number of tokens.\n",
        "# @markdown **max_tokens**: We set the maximum number of tokens.\n",
        "max_tokens = 550 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Changing **summarization_prompt_template** and **summarization_prompt_refine_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "\n",
        "# Initial prompt.\n",
        "# @markdown **summarization_prompt_template**: Initial prompt.\n",
        "summarization_prompt_template = \"請在 300 字以內，提供以下文字的簡潔摘要:<text>\" # @param {type:\"string\"}\n",
        "\n",
        "# Refinement prompt.\n",
        "# @markdown **summarization_prompt_refinement_template**: Refinement prompt.\n",
        "summarization_prompt_refinement_template = \"請在 500 字以內，結合原先的摘要和新的內容，提供簡潔的摘要:<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEtfC9WeZkNH"
      },
      "source": [
        "The code block below takes about **200** seconds to run when using the (1) **gpt-3.5-turbo** model and (2) maximum number of tokens is 500, but the actual time may vary depending on the condition of Colab and the status of the OpenAI API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rugUlJ6HeZPF"
      },
      "source": [
        "Pipeline of the method of Refinement.\n",
        "\n",
        "Step1: It starts by running a prompt on a small portion of the data, generating initial output.\n",
        "\n",
        "Step2: For each following document, the previous output is fed in along with the new document.\n",
        "\n",
        "Step3: The LLM is instructed to refine the output based on the new document's information.\n",
        "\n",
        "Step4: This process continues iteratively until all documents have been processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aFmk1ATAOdd"
      },
      "outputs": [],
      "source": [
        "paragraph_summarizations = []\n",
        "\n",
        "# First, we summarize each section that has been split up separately.\n",
        "for index, chunk in enumerate(chunks):\n",
        "\n",
        "    if index == 0:\n",
        "        # Record the start time.\n",
        "        start = time.time()\n",
        "\n",
        "        # Construct summarization prompt.\n",
        "        summarization_prompt = summarization_prompt_template.replace(\"<text>\", chunk)\n",
        "\n",
        "        # Step1: It starts by running a prompt on a small portion of the data, generating initial output.\n",
        "        first_paragraph_summarization = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "        # Record the result.\n",
        "        paragraph_summarizations.append(first_paragraph_summarization)\n",
        "\n",
        "        # Calculate the execution time and round it to 2 decimal places.\n",
        "        cost_time = round(time.time() - start, 2)\n",
        "\n",
        "        # Print the summary and its length.\n",
        "        print(f\"----------------------------Summary of Segment {index + 1}----------------------------\\n\")\n",
        "        for text in textwrap.wrap(first_paragraph_summarization, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "        print(f\"Length of summary for segment {index + 1}: {len(first_paragraph_summarization)}\")\n",
        "        print(f\"Time taken to generate summary for segment {index + 1}: {cost_time} sec.\\n\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        # Record the start time.\n",
        "        start = time.time()\n",
        "\n",
        "        # Step2: For each following document, the previous output is fed in along with the new document.\n",
        "        chunk_text = f\"\"\"前 {index} 段的摘要: {paragraph_summarizations[-1]}\\n第 {index + 1} 段的內容: {chunk}\"\"\"\n",
        "\n",
        "        # Construct refinement prompt for summarization.\n",
        "        summarization_prompt = summarization_prompt_refinement_template.replace(\"<text>\", chunk_text)\n",
        "\n",
        "        # Step3: The LLM is instructed to refine the output based on the new document's information.\n",
        "        paragraph_summarization = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "        # Record the result.\n",
        "        paragraph_summarizations.append(paragraph_summarization)\n",
        "\n",
        "        # Calculate the execution time and round it to 2 decimal places.\n",
        "        cost_time = round(time.time() - start, 2)\n",
        "\n",
        "        # print results.\n",
        "        print(f\"----------------------------Summary of the First {index + 1} Segments----------------------------\\n\")\n",
        "        for text in textwrap.wrap(paragraph_summarization, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "        print(f\"Length of summary for the first {index + 1} segments: {len(paragraph_summarization)}\")\n",
        "        print(f\"Time taken to generate summary for the first {index + 1} segments: {cost_time} sec.\\n\")\n",
        "\n",
        "    # Step4: This process continues iteratively until all documents have been processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaXWOLIOa4dM"
      },
      "outputs": [],
      "source": [
        "''' In this block, you can modify your desired output path of final summary. '''\n",
        "\n",
        "output_path = f\"./final-summary-{suffix}-chatgpt-refinement.txt\"\n",
        "\n",
        "# If you need to convert Simplified Chinese to Traditional Chinese, please set this option to True; otherwise, set it to False.\n",
        "convert_to_tradition_chinese = False\n",
        "\n",
        "if convert_to_tradition_chinese == True:\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    paragraph_summarizations[-1] = cc.convert(paragraph_summarizations[-1])\n",
        "\n",
        "# Output your final summary\n",
        "with open(output_path, \"w\") as fp:\n",
        "    fp.write(paragraph_summarizations[-1])\n",
        "\n",
        "# Show the result.\n",
        "print(f\"Final summary has been saved to {output_path}\")\n",
        "print(f\"\\n===== Below is the final summary ({len(paragraph_summarizations[-1])} words) =====\\n\")\n",
        "for text in textwrap.wrap(paragraph_summarizations[-1], 80):\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCM4kPuBXh7R"
      },
      "source": [
        "## **If you want to use Gemini, begin with this part.**\n",
        "##### (1) You can refer to https://shorturl.at/X0NDY (Page 35) for obtaining Gemini API key.\n",
        "##### (2) You can refer to https://ai.google.dev/models/gemini for more details about which models you can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anuFrhHNkMgY"
      },
      "outputs": [],
      "source": [
        "def summarization(summarization_prompt, model_name=\"gemini-pro\", temperature=0.0, top_p=1.0, max_tokens=512):\n",
        "    \"\"\"\n",
        "    (1) Objective:\n",
        "        - Use the OpenAI Chat API to summarize a given text.\n",
        "\n",
        "    (2) Arguments:\n",
        "        - summarization_prompt: The summarization prompt.\n",
        "        - model_name: The model name, default is \"gemini-pro\". You can refer to \"https://ai.google.dev/models/gemini\" for more details.\n",
        "        - temperature: Controls randomness in the response. Lower values make responses more deterministic, default is 0.0.\n",
        "        - top_p: Controls diversity via nucleus sampling. Higher values lead to more diverse responses, default is 1.0.\n",
        "        - max_tokens: The maximum number of tokens to generate in the completion, default is 512.\n",
        "\n",
        "    (3) Return:\n",
        "        - The summarized text.\n",
        "\n",
        "    (4) Example:\n",
        "        - If the text is \"ABC\" and the summarization prompt is \"DEF\", model_name is \"gemini-pro\",\n",
        "          temperature is 0.0, top_p is 1.0, and max_tokens is 512, then you can call the function like this:\n",
        "\n",
        "              summarization(text=\"ABC\", summarization_prompt=\"DEF\", model_name=\"gemini-pro\", temperature=0.0, top_p=1.0, max_tokens=512)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # The user prompt is a concatenation of the summarization_prompt and text.\n",
        "    user_prompt = summarization_prompt\n",
        "\n",
        "    # Load the generative model.\n",
        "    model = genai.GenerativeModel(model_name)\n",
        "\n",
        "    # Set the generation configuration.\n",
        "    generation_config = genai.GenerationConfig(temperature=temperature, top_p=top_p, max_output_tokens=max_tokens)\n",
        "\n",
        "    while True:\n",
        "\n",
        "        try:\n",
        "            # Use the OpenAI Chat API to summarize the text.\n",
        "            response = model.generate_content(contents=user_prompt, generation_config=generation_config)\n",
        "\n",
        "            break\n",
        "\n",
        "        except:\n",
        "            # If the API call fails, wait for 1 second and try again.\n",
        "            print(\"The API call fails, wait for 1 second and try again.\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plyvWCvXllz3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Parameter Setting of Gemini { run: \"auto\" }\n",
        "''' In this block, you can modify your desired parameters and set your api key. '''\n",
        "\n",
        "# Your google api key.\n",
        "# @markdown **google_api_key**: Your google api key.\n",
        "google_api_key = \"YOUR_GOOGLE_API_KEY\" # @param {type:\"string\"}\n",
        "\n",
        "# The model name. You can refer to \"https://ai.google.dev/models/gemini\" for more details.\n",
        "# @markdown **model_name**: The model name. You can refer to \"https://ai.google.dev/models/gemini\" for more details.\n",
        "model_name = \"gemini-pro\" # @param {type:\"string\"}\n",
        "\n",
        "# Controls randomness in the response. Lower values make responses more deterministic\n",
        "# @markdown **temperature**: Controls randomness in the response. Lower values make responses more deterministic.\n",
        "temperature = 0.0 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "# Controls diversity via nucleus sampling. Higher values lead to more diverse responses\n",
        "# @markdown **top_p**: Controls diversity via nucleus sampling. Higher values lead to more diverse responses.\n",
        "top_p = 1.0 # @param {type:\"slider\", min:0, max:1, step:0.1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eed0NjqJtGfM"
      },
      "outputs": [],
      "source": [
        "# Set Google API key.\n",
        "genai.configure(api_key=google_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csw7hxrHsJym"
      },
      "source": [
        "### We offer the following two methods for summarization.\n",
        "Reference: https://reurl.cc/VzagLA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGHFGCyasp9h"
      },
      "source": [
        "#### **If you want to use the method of Multi-Stage Summarization, begin with this part.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TsX6dBgs1iw"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of Gemini Multi-Stage Summarization: Paragraph { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# The maximum number of tokens to generate in the completion.\n",
        "# @markdown **max_tokens**: The maximum number of tokens to generate in the completion.\n",
        "max_tokens = 350 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown #### Changing **summarization_prompt_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "summarization_prompt_template = \"用 300 個字內寫出這段文字的摘要，其中包括要點和所有重要細節：<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkQdOOixuJU9"
      },
      "source": [
        "##### Step1: Split the long text into multiple smaller pieces and obtain summaries for each smaller text piece separately\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olrUkpe615-E"
      },
      "source": [
        "The code block below takes about **40** seconds to run when using the (1) **gemini-pro** model, (2) length of chunks is 512 and (3) maximum number of tokens is 350, but the actual time may vary depending on the condition of Colab and the status of the Google API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KOp_d6XloCn"
      },
      "outputs": [],
      "source": [
        "paragraph_summarizations = []\n",
        "\n",
        "# First, we summarize each section that has been split up separately.\n",
        "for index, chunk in enumerate(chunks):\n",
        "\n",
        "    # Record the start time.\n",
        "    start = time.time()\n",
        "\n",
        "    # Construct summarization prompt.\n",
        "    summarization_prompt = summarization_prompt_template.replace(\"<text>\", chunk)\n",
        "\n",
        "    # We summarize each section that has been split up separately.\n",
        "    response = summarization(summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "    # Calculate the execution time and round it to 2 decimal places.\n",
        "    cost_time = round(time.time() - start, 2)\n",
        "\n",
        "    # Print the summary and its length.\n",
        "    print(f\"----------------------------Summary of Segment {index + 1}----------------------------\\n\")\n",
        "    for text in textwrap.wrap(response, 80):\n",
        "        print(f\"{text}\\n\")\n",
        "    print(f\"Length of summary for segment {index + 1}: {len(response)}\")\n",
        "    print(f\"Time taken to generate summary for segment {index + 1}: {cost_time} sec.\\n\")\n",
        "\n",
        "    # Record the result.\n",
        "    paragraph_summarizations.append(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amp-gOibmpvK"
      },
      "outputs": [],
      "source": [
        "# First, we collect all the summarizations obtained before and print them.\n",
        "\n",
        "collected_summarization = \"\"\n",
        "for index, paragraph_summarization in enumerate(paragraph_summarizations):\n",
        "    collected_summarization += f\"Summary of segment {index + 1}: {paragraph_summarization}\\n\"\n",
        "\n",
        "print(collected_summarization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y08WjQxiuZ0k"
      },
      "source": [
        "#####Step2: After obtaining summaries for each smaller text piece separately, process these summaries to generate the final summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4GEhPEIudBe"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of Gemini Multi-Stage Summarization: Total { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# We set the maximum number of tokens to ensure that the final summary does not exceed 550 tokens.\n",
        "# @markdown **max_tokens**: We set the maximum number of tokens to ensure that the final summary does not exceed 550 tokens.\n",
        "max_tokens = 550 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Changing **summarization_prompt_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "summarization_prompt_template = \"在 500 字以內寫出以下文字的簡潔摘要：<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdTjmyvfZwar"
      },
      "source": [
        "The code block below takes about **20** seconds to run when using the (1) **gemini-pro** model, (2) length of chunks is 512 and (3) maximum number of tokens is 550, but the actual time may vary depending on the condition of Colab and the status of the Google API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6x5fn6I-msGC"
      },
      "outputs": [],
      "source": [
        "# Finally, we compile a final summary from the summaries of each section.\n",
        "\n",
        "# Record the start time.\n",
        "start = time.time()\n",
        "\n",
        "# Run final summarization.\n",
        "summarization_prompt = summarization_prompt_template.replace(\"<text>\", collected_summarization)\n",
        "final_summarization = summarization(summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "# Calculate the execution time and round it to 2 decimal places.\n",
        "cost_time = round(time.time() - start, 2)\n",
        "\n",
        "# Print the summary and its length.\n",
        "print(f\"----------------------------Final Summary----------------------------\\n\")\n",
        "for text in textwrap.wrap(final_summarization, 80):\n",
        "        print(f\"{text}\")\n",
        "print(f\"\\nLength of final summary: {len(final_summarization)}\")\n",
        "print(f\"Time taken to generate the final summary: {cost_time} sec.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPR2Am6omt0U"
      },
      "outputs": [],
      "source": [
        "''' In this block, you can modify your desired output path of final summary. '''\n",
        "\n",
        "output_path = f\"./final-summary-{suffix}-gemini-multi-stage.txt\"\n",
        "\n",
        "# If you need to convert Simplified Chinese to Traditional Chinese, please set this option to True; otherwise, set it to False.\n",
        "convert_to_tradition_chinese = False\n",
        "\n",
        "if convert_to_tradition_chinese == True:\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    final_summarization = cc.convert(final_summarization)\n",
        "\n",
        "# Output your final summary\n",
        "with open(output_path, \"w\") as fp:\n",
        "    fp.write(final_summarization)\n",
        "\n",
        "print(f\"Final summary has been saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDZVeHCKvcMy"
      },
      "source": [
        "#### **If you want to use the method of Refinement, begin with this part.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1goEansHvc8C"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of Gemini Refinement { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# We set the maximum number of tokens.\n",
        "# @markdown **max_tokens**: We set the maximum number of tokens.\n",
        "max_tokens = 550 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Changing **summarization_prompt_template** and **summarization_prompt_refine_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "\n",
        "# Initial prompt.\n",
        "# @markdown **summarization_prompt_template**: Initial prompt.\n",
        "summarization_prompt_template = \"請在 300 字以內，提供以下文字的簡潔摘要:<text>\" # @param {type:\"string\"}\n",
        "\n",
        "# Refinement prompt.\n",
        "# @markdown **summarization_prompt_refinement_template**: Refinement prompt.\n",
        "summarization_prompt_refinement_template = \"請在 500 字以內，結合原先的摘要和新的內容，提供簡潔的摘要:<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5AunhcUwDWh"
      },
      "source": [
        "Pipeline of the method of Refinement.\n",
        "\n",
        "Step1: It starts by running a prompt on a small portion of the data, generating initial output.\n",
        "\n",
        "Step2: For each following document, the previous output is fed in along with the new document.\n",
        "\n",
        "Step3: The LLM is instructed to refine the output based on the new document's information.\n",
        "\n",
        "Step4: This process continues iteratively until all documents have been processed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJMgqTC_2WD0"
      },
      "source": [
        "The code block below takes about **45** seconds to run when using the (1) **gemini-pro** model and (2) maximum number of tokens is 500, but the actual time may vary depending on the condition of Colab and the status of the Google API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDWW_K4OwG1N"
      },
      "outputs": [],
      "source": [
        "paragraph_summarizations = []\n",
        "\n",
        "# First, we summarize each section that has been split up separately.\n",
        "for index, chunk in enumerate(chunks):\n",
        "\n",
        "    if index == 0:\n",
        "        # Record the start time.\n",
        "        start = time.time()\n",
        "\n",
        "        # Construct summarization prompt.\n",
        "        summarization_prompt = summarization_prompt_template.replace(\"<text>\", chunk)\n",
        "\n",
        "        # Step1: It starts by running a prompt on a small portion of the data, generating initial output.\n",
        "        first_paragraph_summarization = summarization(summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "        # Record the result.\n",
        "        paragraph_summarizations.append(first_paragraph_summarization)\n",
        "\n",
        "        # Calculate the execution time and round it to 2 decimal places.\n",
        "        cost_time = round(time.time() - start, 2)\n",
        "\n",
        "        # Print the summary and its length.\n",
        "        print(f\"----------------------------Summary of Segment {index + 1}----------------------------\\n\")\n",
        "        for text in textwrap.wrap(first_paragraph_summarization, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "        print(f\"Length of summary for segment {index + 1}: {len(first_paragraph_summarization)}\")\n",
        "        print(f\"Time taken to generate summary for segment {index + 1}: {cost_time} sec.\\n\")\n",
        "\n",
        "    else:\n",
        "        # Record the start time.\n",
        "        start = time.time()\n",
        "\n",
        "        # Step2: For each following document, the previous output is fed in along with the new document.\n",
        "        chunk_text = f\"\"\"前 {index} 段的摘要: {paragraph_summarizations[-1]}\\n第 {index + 1} 段的內容: {chunk}\"\"\"\n",
        "\n",
        "        # Construct refinement prompt for summarization.\n",
        "        summarization_prompt = summarization_prompt_refinement_template.replace(\"<text>\", chunk_text)\n",
        "\n",
        "        # Step3: The LLM is instructed to refine the output based on the new document's information.\n",
        "        paragraph_summarization = summarization(summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "        # Record the result.\n",
        "        paragraph_summarizations.append(paragraph_summarization)\n",
        "\n",
        "        # Calculate the execution time and round it to 2 decimal places.\n",
        "        cost_time = round(time.time() - start, 2)\n",
        "\n",
        "        # print results.\n",
        "        print(f\"----------------------------Summary of the First {index + 1} Segments----------------------------\\n\")\n",
        "        for text in textwrap.wrap(paragraph_summarization, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "        print(f\"Length of summary for the first {index + 1} segments: {len(paragraph_summarization)}\")\n",
        "        print(f\"Time taken to generate summary for the first {index + 1} segments: {cost_time} sec.\\n\")\n",
        "\n",
        "    # Step4: This process continues iteratively until all documents have been processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5jw0HwWwW83"
      },
      "outputs": [],
      "source": [
        "''' In this block, you can modify your desired output path of final summary. '''\n",
        "\n",
        "output_path = f\"./final-summary-{suffix}-gemini-refinement.txt\"\n",
        "\n",
        "# If you need to convert Simplified Chinese to Traditional Chinese, please set this option to True; otherwise, set it to False.\n",
        "convert_to_tradition_chinese = False\n",
        "\n",
        "if convert_to_tradition_chinese == True:\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    paragraph_summarizations[-1] = cc.convert(paragraph_summarizations[-1])\n",
        "\n",
        "# Output your final summary\n",
        "with open(output_path, \"w\") as fp:\n",
        "    fp.write(paragraph_summarizations[-1])\n",
        "\n",
        "# Show the result.\n",
        "print(f\"Final summary has been saved to {output_path}\")\n",
        "print(f\"\\n===== Below is the final summary ({len(paragraph_summarizations[-1])} words) =====\\n\")\n",
        "for text in textwrap.wrap(paragraph_summarizations[-1], 64):\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3rwvZYdXieB"
      },
      "source": [
        "## **If you want to use Claude, begin with this part.**\n",
        "##### (1) You can refer to https://reurl.cc/yLy06D for obtaining Claude API key.\n",
        "##### (2) You can refer to https://docs.anthropic.com/claude/docs/models-overview for more details about which models you can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-51sj3lXieC"
      },
      "outputs": [],
      "source": [
        "def summarization(client, summarization_prompt, model_name=\"claude-3-sonnet-20240229\", temperature=0.0, top_p=1.0, max_tokens=512):\n",
        "    \"\"\"\n",
        "    (1) Objective:\n",
        "        - Use the Claude API to summarize a given text.\n",
        "\n",
        "    (2) Arguments:\n",
        "        - client: Claude API client.\n",
        "        - text: The text to be summarized.\n",
        "        - summarization_prompt: The summarization prompt.\n",
        "        - model_name: The model name, default is \"claude-3-sonnet-20240229\". You can refer to \"https://docs.anthropic.com/claude/docs/models-overview#model-comparison\" for more details.\n",
        "        - temperature: Controls randomness in the response. Lower values make responses more deterministic, default is 0.0.\n",
        "        - top_p: Controls diversity via nucleus sampling. Higher values lead to more diverse responses, default is 1.0.\n",
        "        - max_tokens: The maximum number of tokens to generate in the completion, default is 512.\n",
        "\n",
        "    (3) Return:\n",
        "        - The summarized text.\n",
        "\n",
        "    (4) Example:\n",
        "        - If the text is \"ABC\" and the summarization prompt is \"DEF\", system_prompt is \"GHI\", model_name is \"claude-3-sonnet-20240229\",\n",
        "          temperature is 0.0, top_p is 1.0, and max_tokens is 512, then you can call the function like this:\n",
        "\n",
        "              summarization(client=client, text=\"ABC\", summarization_prompt=\"DEF\", system_prompt=\"GHI\", model_name=\"claude-3-sonnet-20240229\", temperature=0.0, top_p=1.0, max_tokens=512)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = summarization_prompt\n",
        "\n",
        "    while True:\n",
        "\n",
        "        try:\n",
        "            # Use the Claude API to summarize the text.\n",
        "            message = client.messages.create(\n",
        "                model=model_name,\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=temperature,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            break\n",
        "\n",
        "        except:\n",
        "            # If the API call fails, wait for 1 second and try again.\n",
        "            print(\"The API call fails, wait for 1 second and try again.\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    return message.content[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mfcYh1Rwkrh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Parameter Setting of Claude { run: \"auto\" }\n",
        "''' ===== In this block, you can modify your desired parameters and set your Claude API key ===== '''\n",
        "\n",
        "# Your Claude API key.\n",
        "# @markdown **claude_api_key**: Your Claude api key.\n",
        "claude_api_key = \"YOUR_CLAUDE_API_KEY\" # @param {type:\"string\"}\n",
        "\n",
        "# The model name, default is \"claude-3-opus-20240229\". You can refer to \"https://docs.anthropic.com/claude/docs/models-overview#model-comparison\" for more details.\n",
        "# @markdown **model_name**: The model name, default is \"claude-3-opus-20240229\". You can refer to \"https://docs.anthropic.com/claude/docs/models-overview#model-comparison\" for more details.\n",
        "model_name = \"claude-3-opus-20240229\" # @param {type:\"string\"}\n",
        "\n",
        "# Controls randomness in the response. Lower values make responses more deterministic.\n",
        "# @markdown **temperature**: Controls randomness in the response. Lower values make responses more deterministic.\n",
        "temperature = 1 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "# Controls diversity via nucleus sampling. Higher values lead to more diverse responses.\n",
        "# @markdown **top_p**: Controls diversity via nucleus sampling. Higher values lead to more diverse responses.\n",
        "top_p = 1.0 # @param {type:\"slider\", min:0, max:1, step:0.1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-ivbHl6wrIj"
      },
      "outputs": [],
      "source": [
        "# Construct Claude client.\n",
        "client = anthropic.Anthropic(api_key=claude_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eDZJpX7xagQ"
      },
      "source": [
        "### We offer the following two methods for summarization.\n",
        "Reference: https://reurl.cc/VzagLA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEmYimdXxctB"
      },
      "source": [
        "#### **If you want to use the method of Multi-Stage Summarization, begin with this part.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4OBcswSxfUp"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of Claude Multi-Stage Summarization: Paragraph { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# The maximum number of tokens to generate in the completion.\n",
        "# @markdown **max_tokens**: The maximum number of tokens to generate in the completion.\n",
        "max_tokens = 350 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown #### Changing **summarization_prompt_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "summarization_prompt_template = \"用 300 個字內寫出這段文字的摘要，其中包括要點和所有重要細節：<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WClFqWBsyUWB"
      },
      "source": [
        "##### Step1: Split the long text into multiple smaller pieces and obtain summaries for each smaller text piece separately"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avQs_YFg2mK7"
      },
      "source": [
        "The code block below takes about **120** seconds to run when using the (1) **claude-3-opus-20240229** model, (2) length of chunks is 512 and (3) maximum number of tokens is 350, but the actual time may vary depending on the condition of Colab and the status of the Claude API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJlEpq4EqIAN"
      },
      "outputs": [],
      "source": [
        "paragraph_summarizations = []\n",
        "\n",
        "# First, we summarize each section that has been split up separately.\n",
        "for index, chunk in enumerate(chunks):\n",
        "\n",
        "    # Record the start time.\n",
        "    start = time.time()\n",
        "\n",
        "    # Construct summarization prompt.\n",
        "    summarization_prompt = summarization_prompt_template.replace(\"<text>\", chunk)\n",
        "\n",
        "    # We summarize each section that has been split up separately.\n",
        "    response = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "    # Calculate the execution time and round it to 2 decimal places.\n",
        "    cost_time = round(time.time() - start, 2)\n",
        "\n",
        "    # Print the summary and its length.\n",
        "    print(f\"----------------------------Summary of Segment {index + 1}----------------------------\\n\")\n",
        "    for text in textwrap.wrap(response, 80):\n",
        "        print(f\"{text}\\n\")\n",
        "    print(f\"Length of summary for segment {index + 1}: {len(response)}\")\n",
        "    print(f\"Time taken to generate summary for segment {index + 1}: {cost_time} sec.\\n\")\n",
        "\n",
        "    # Record the result.\n",
        "    paragraph_summarizations.append(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vzgd8rWqJP9"
      },
      "outputs": [],
      "source": [
        "# First, we collect all the summarizations obtained before and print them.\n",
        "\n",
        "collected_summarization = \"\"\n",
        "for index, paragraph_summarization in enumerate(paragraph_summarizations):\n",
        "    collected_summarization += f\"Summary of segment {index + 1}: {paragraph_summarization}\\n\\n\"\n",
        "\n",
        "print(collected_summarization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qDxoyLpymA5"
      },
      "source": [
        "##### Step2: After obtaining summaries for each smaller text piece separately, process these summaries to generate the final summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E99bQfbgynXT"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of Gemini Multi-Stage Summarization: Total { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# We set the maximum number of tokens to ensure that the final summary does not exceed 550 tokens.\n",
        "# @markdown **max_tokens**: We set the maximum number of tokens to ensure that the final summary does not exceed 550 tokens.\n",
        "max_tokens = 550 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Changing **summarization_prompt_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "summarization_prompt_template = \"在 500 字以內寫出以下文字的簡潔摘要：<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-WvuFl4XV2J"
      },
      "source": [
        "The code block below takes about **25** seconds to run when using the (1) **claude-3-opus-20240229** model, (2) length of chunks is 512 and (3) maximum number of tokens is 550, but the actual time may vary depending on the condition of Colab and the status of the Claude API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qX4pa5QqKrc"
      },
      "outputs": [],
      "source": [
        "# Finally, we compile a final summary from the summaries of each section.\n",
        "\n",
        "# Record the start time.\n",
        "start = time.time()\n",
        "\n",
        "summarization_prompt = summarization_prompt_template.replace(\"<text>\", collected_summarization)\n",
        "\n",
        "# Run final summarization.\n",
        "final_summarization = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "# Calculate the execution time and round it to 2 decimal places.\n",
        "cost_time = round(time.time() - start, 2)\n",
        "\n",
        "# Print the summary and its length.\n",
        "print(f\"----------------------------Final Summary----------------------------\\n\")\n",
        "for text in textwrap.wrap(final_summarization, 80):\n",
        "    print(f\"{text}\")\n",
        "print(f\"\\nLength of final summary: {len(final_summarization)}\")\n",
        "print(f\"Time taken to generate the final summary: {cost_time} sec.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldTq9WYlqL2U"
      },
      "outputs": [],
      "source": [
        "''' In this block, you can modify your desired output path of final summary. '''\n",
        "\n",
        "output_path = f\"./final-summary-{suffix}-claude-multi-stage.txt\"\n",
        "\n",
        "# If you need to convert Simplified Chinese to Traditional Chinese, please set this option to True; otherwise, set it to False.\n",
        "convert_to_tradition_chinese = False\n",
        "\n",
        "if convert_to_tradition_chinese == True:\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    final_summarization = cc.convert(final_summarization)\n",
        "\n",
        "# Output your final summary\n",
        "with open(output_path, \"w\") as fp:\n",
        "    fp.write(final_summarization)\n",
        "\n",
        "# Show the result.\n",
        "print(f\"Final summary has been saved to {output_path}\")\n",
        "print(f\"\\n===== Below is the final summary ({len(final_summarization)} words) =====\\n\")\n",
        "for text in textwrap.wrap(final_summarization, 64):\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBI-Ba8QzLU8"
      },
      "source": [
        "#### **If you want to use the method of Refinement, begin with this part.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t2muOwUzNd0"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of Claude Refinement { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# We set the maximum number of tokens.\n",
        "# @markdown **max_tokens**: We set the maximum number of tokens.\n",
        "max_tokens = 550 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Changing **summarization_prompt_template** and **summarization_prompt_refine_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "\n",
        "# Initial prompt.\n",
        "# @markdown **summarization_prompt_template**: Initial prompt.\n",
        "summarization_prompt_template = \"請在 300 字以內，提供以下文字的簡潔摘要:<text>\" # @param {type:\"string\"}\n",
        "\n",
        "# Refinement prompt.\n",
        "# @markdown **summarization_prompt_refinement_template**: Refinement prompt.\n",
        "summarization_prompt_refinement_template = \"請在 500 字以內，結合原先的摘要和新的內容，提供簡潔的摘要:<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JxW1AR2zZDL"
      },
      "source": [
        "Pipeline of the method of Refinement.\n",
        "\n",
        "Step1: It starts by running a prompt on a small portion of the data, generating initial output.\n",
        "\n",
        "Step2: For each following document, the previous output is fed in along with the new document.\n",
        "\n",
        "Step3: The LLM is instructed to refine the output based on the new document's information.\n",
        "\n",
        "Step4: This process continues iteratively until all documents have been processed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkm60apd2tQd"
      },
      "source": [
        "The code block below takes about **150** seconds to run when using the (1) **claude-3-opus-20240229** model and (2) maximum number of tokens is 500, but the actual time may vary depending on the condition of Colab and the status of the Claude API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Feco4LK4zazs"
      },
      "outputs": [],
      "source": [
        "paragraph_summarizations = []\n",
        "\n",
        "# First, we summarize each section that has been split up separately.\n",
        "for index, chunk in enumerate(chunks):\n",
        "\n",
        "    if index == 0:\n",
        "        # Record the start time.\n",
        "        start = time.time()\n",
        "\n",
        "        # Construct summarization prompt.\n",
        "        summarization_prompt = summarization_prompt_template.replace(\"<text>\", chunk)\n",
        "\n",
        "        # Step1: It starts by running a prompt on a small portion of the data, generating initial output.\n",
        "        first_paragraph_summarization = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "        # Record the result.\n",
        "        paragraph_summarizations.append(first_paragraph_summarization)\n",
        "\n",
        "        # Calculate the execution time and round it to 2 decimal places.\n",
        "        cost_time = round(time.time() - start, 2)\n",
        "\n",
        "        # Print the summary and its length.\n",
        "        print(f\"----------------------------Summary of Segment {index + 1}----------------------------\\n\")\n",
        "        for text in textwrap.wrap(response, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "        print(f\"Length of summary for segment {index + 1}: {len(response)}\")\n",
        "        print(f\"Time taken to generate summary for segment {index + 1}: {cost_time} sec.\\n\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        # Record the start time.\n",
        "        start = time.time()\n",
        "\n",
        "        # Step2: For each following document, the previous output is fed in along with the new document.\n",
        "        chunk_text = f\"\"\"前 {index} 段的摘要: {paragraph_summarizations[-1]}\\n第 {index + 1} 段的內容: {chunk}\"\"\"\n",
        "\n",
        "        # Construct refinement prompt for summarization.\n",
        "        summarization_prompt = summarization_prompt_refinement_template.replace(\"<text>\", chunk_text)\n",
        "\n",
        "        # Step3: The LLM is instructed to refine the output based on the new document's information.\n",
        "        paragraph_summarization = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "        # Record the result.\n",
        "        paragraph_summarizations.append(paragraph_summarization)\n",
        "\n",
        "        # Calculate the execution time and round it to 2 decimal places.\n",
        "        cost_time = round(time.time() - start, 2)\n",
        "\n",
        "        # print results.\n",
        "        print(f\"----------------------------Summary of the First {index + 1} Segments----------------------------\\n\")\n",
        "        for text in textwrap.wrap(paragraph_summarization, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "        print(f\"Length of summary for the first {index + 1} segments: {len(paragraph_summarization)}\")\n",
        "        print(f\"Time taken to generate summary for the first {index + 1} segments: {cost_time} sec.\\n\")\n",
        "\n",
        "    # Step4: This process continues iteratively until all documents have been processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4BGRgvFz0eW"
      },
      "outputs": [],
      "source": [
        "''' In this block, you can modify your desired output path of final summary. '''\n",
        "\n",
        "output_path = f\"./final-summary-{suffix}-claude-refinement.txt\"\n",
        "\n",
        "# If you need to convert Simplified Chinese to Traditional Chinese, please set this option to True; otherwise, set it to False.\n",
        "convert_to_tradition_chinese = False\n",
        "\n",
        "if convert_to_tradition_chinese == True:\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    paragraph_summarizations[-1] = cc.convert(paragraph_summarizations[-1])\n",
        "\n",
        "# Output your final summary\n",
        "with open(output_path, \"w\") as fp:\n",
        "    fp.write(paragraph_summarizations[-1])\n",
        "\n",
        "# Show the result.\n",
        "print(f\"Final summary has been saved to {output_path}\")\n",
        "print(f\"\\n===== Below is the final summary ({len(paragraph_summarizations[-1])} words) =====\\n\")\n",
        "for text in textwrap.wrap(paragraph_summarizations[-1], 80):\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtQoyYjfdQLC"
      },
      "source": [
        "# Part5 - Check the correctness of the submission file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gclg3cORdiVR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a9137b5-b442-492e-af61-f687fbf2be0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The format of your submission file is correct.\n",
            "Your final score is 0.\n"
          ]
        }
      ],
      "source": [
        "# Check the correctness of the submission file.\n",
        "import json\n",
        "import re\n",
        "\n",
        "your_submission_path = \"YOUR_SUBMISSION_PATH\"\n",
        "\n",
        "def check_format(your_submission_path):\n",
        "\n",
        "    final_score = 0\n",
        "\n",
        "    # check the extension of the file.\n",
        "    if not your_submission_path.endswith(\".json\"):\n",
        "        print(\"Please save your submission file in JSON format.\")\n",
        "        return False, final_score\n",
        "    else:\n",
        "        try:\n",
        "            with open(your_submission_path, \"r\") as fp:\n",
        "                your_submission = json.load(fp)\n",
        "\n",
        "            evaluation_result = your_submission[\"history\"][0][\"messages\"][1][\"content\"]\n",
        "\n",
        "            if \"總分：\" not in evaluation_result:\n",
        "                # Correct format: 總分: <你的分數>\n",
        "                print(\"Please make sure that the correct format of final score is included in the evaluation result.\")\n",
        "                print(\"The correct format is 總分: <你的分數>. For example, 總分: 97\")\n",
        "                return False, final_score\n",
        "\n",
        "            evaluation_result = evaluation_result.strip()\n",
        "            score_pattern = r\"總分：\\d+\"\n",
        "            score = re.findall(score_pattern, evaluation_result)\n",
        "\n",
        "            if score:\n",
        "                final_score = score[-1].replace(\"總分：\", \"\")\n",
        "                if \"/100\" in final_score:\n",
        "                    final_score = final_score.replace(\"/100\", \"\")\n",
        "            else:\n",
        "                print(\"Please make sure that the final score is included in the evaluation result.\")\n",
        "                return False, final_score\n",
        "\n",
        "        except:\n",
        "            print(\"Open the file failed. Please check the file path or save your submission file in correct JSON format\")\n",
        "            return False, final_score\n",
        "\n",
        "    return True, final_score\n",
        "\n",
        "format_correctness, final_score = check_format(your_submission_path)\n",
        "if format_correctness== True:\n",
        "    print(\"The format of your submission file is correct.\")\n",
        "    print(f\"Your final score is {final_score}.\")\n",
        "else:\n",
        "    print(\"The format of your submission file is wrong.\")\n",
        "    print(\"Please check the format of your submission file.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "z9QC8lG_QRZL",
        "hRzf_0cTV6TS",
        "GEmYimdXxctB",
        "eBI-Ba8QzLU8"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07fb96681a794a2a9b22fbf7c0a4f6db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f34be7b48f8d4f1d8b2facdcd92760c6",
              "IPY_MODEL_351df89abd2b4da9b608cbfc83dfaf55",
              "IPY_MODEL_3bd2f0a1ca004d64b64b8af29a97d785"
            ],
            "layout": "IPY_MODEL_a8e9182146344d099749e6309d567ba0"
          }
        },
        "f34be7b48f8d4f1d8b2facdcd92760c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c05d52994f9a44419971a981ac78ac86",
            "placeholder": "​",
            "style": "IPY_MODEL_6b746f9ca5984260bf9af6c0aa9e24a5",
            "value": "Downloading readme: 100%"
          }
        },
        "351df89abd2b4da9b608cbfc83dfaf55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae81ac0896e4450c8f81a700f1027bdf",
            "max": 305,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af14c439d0d648729e93522034cb3b25",
            "value": 305
          }
        },
        "3bd2f0a1ca004d64b64b8af29a97d785": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb8f40f8a208435cb74102065b00a566",
            "placeholder": "​",
            "style": "IPY_MODEL_423906d4a7d74189a5ac9b60bcdded2a",
            "value": " 305/305 [00:00&lt;00:00, 17.4kB/s]"
          }
        },
        "a8e9182146344d099749e6309d567ba0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c05d52994f9a44419971a981ac78ac86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b746f9ca5984260bf9af6c0aa9e24a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae81ac0896e4450c8f81a700f1027bdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af14c439d0d648729e93522034cb3b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb8f40f8a208435cb74102065b00a566": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "423906d4a7d74189a5ac9b60bcdded2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54c07b1ebdeb4f6dbf8bda8b387b91e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33a929e2ebfc44428bce63a8dc91af9b",
              "IPY_MODEL_46ca7fd4e5ca418e926b34650a99f695",
              "IPY_MODEL_d0dfc1ea848a46e4b62ed9cf1baca771"
            ],
            "layout": "IPY_MODEL_2e75157afa984d5e99272b7f527b22cb"
          }
        },
        "33a929e2ebfc44428bce63a8dc91af9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_986955dac4644a7b9e2a7a58e09ed2fa",
            "placeholder": "​",
            "style": "IPY_MODEL_56e89d31ac4e46b288157f37fd90b140",
            "value": "Downloading data: 100%"
          }
        },
        "46ca7fd4e5ca418e926b34650a99f695": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb27db333e2e407bb6afe143633e352f",
            "max": 3140168,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_262761b6efe3441c9f89a2924764c4a1",
            "value": 3140168
          }
        },
        "d0dfc1ea848a46e4b62ed9cf1baca771": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf0b23e855fb45b580109a1a367913c7",
            "placeholder": "​",
            "style": "IPY_MODEL_20414fae89ba4d1095c3680c84c5305f",
            "value": " 3.14M/3.14M [00:00&lt;00:00, 9.10MB/s]"
          }
        },
        "2e75157afa984d5e99272b7f527b22cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "986955dac4644a7b9e2a7a58e09ed2fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56e89d31ac4e46b288157f37fd90b140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb27db333e2e407bb6afe143633e352f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "262761b6efe3441c9f89a2924764c4a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf0b23e855fb45b580109a1a367913c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20414fae89ba4d1095c3680c84c5305f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05fb2ea3c7724f0ebfcb5695557fff26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d92966e4fcec4596bc24bd684fa63151",
              "IPY_MODEL_8e3043bdcab6475c852d28607cdf79da",
              "IPY_MODEL_ad3d4f71a3314d8c93be997b094d0bdf"
            ],
            "layout": "IPY_MODEL_a77cf5367a2041218aba15342f528631"
          }
        },
        "d92966e4fcec4596bc24bd684fa63151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3930aa2682464bc3b2f6cd77e0801487",
            "placeholder": "​",
            "style": "IPY_MODEL_b1a1e562fb0044b6ae4bfb7d44cd1a2a",
            "value": "Generating test split: 100%"
          }
        },
        "8e3043bdcab6475c852d28607cdf79da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f01756ebda6b433fa3fc34c87f3883d7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed88eba5dada4b5b8d8577d30a5f1354",
            "value": 1
          }
        },
        "ad3d4f71a3314d8c93be997b094d0bdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5975f20095a840dea0c2704a8ff6d1a1",
            "placeholder": "​",
            "style": "IPY_MODEL_b7f80821c54e4e4bb396da127f29c71f",
            "value": " 1/1 [00:00&lt;00:00, 15.05 examples/s]"
          }
        },
        "a77cf5367a2041218aba15342f528631": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3930aa2682464bc3b2f6cd77e0801487": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1a1e562fb0044b6ae4bfb7d44cd1a2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f01756ebda6b433fa3fc34c87f3883d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed88eba5dada4b5b8d8577d30a5f1354": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5975f20095a840dea0c2704a8ff6d1a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7f80821c54e4e4bb396da127f29c71f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}